# Pipeline Command Implementation Plan

## ğŸ¯ Overview

Add a new `pipeline` command to Oxide Flow CLI for comprehensive pipeline management. This will provide users with tools to discover, create, test, and manage their data pipelines.

## ğŸ“‹ Command Structure

```bash
oxide_flow pipeline <SUBCOMMAND> [OPTIONS]
```

## ğŸ› ï¸ Subcommands

### 1. `list` - List Available Pipelines

**Syntax:**
```bash
oxide_flow pipeline list [OPTIONS]
```

**Options:**
- `--tags <TAGS>` / `-t` - Filter by tags (comma-separated)
- `--filter <KEYWORD>` / `-f` - Filter by keyword in name/description
- `--verbose` / `-v` - Show detailed information

**Features:**
- Discovers all pipelines in configured pipeline directory
- Displays pipeline metadata (name, description, version, author, tags)
- Shows step count and basic pipeline information
- Color-coded output for easy reading
- Search and filtering capabilities

**Example Output:**
```
ğŸ“‚ Available pipelines in ./pipelines (5 total):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Name                â”‚ Description                  â”‚ Version â”‚ Steps     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ pipeline            â”‚ JSON to CSV Converter        â”‚ v1.0.0  â”‚ 4 steps   â”‚
â”‚ data_processor      â”‚ Customer Data ETL            â”‚ v2.1.0  â”‚ 7 steps   â”‚
â”‚ error_handling_test â”‚ Error Recovery Demo          â”‚ v1.0.0  â”‚ 5 steps   â”‚
â”‚ file_test           â”‚ Simple File Test             â”‚ v1.0.0  â”‚ 3 steps   â”‚
â”‚ json_flattener      â”‚ Flatten Nested JSON         â”‚ v1.2.0  â”‚ 4 steps   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’¡ Use 'oxide_flow pipeline info <name>' for detailed information
ğŸš€ Use 'oxide_flow run <name>' to execute a pipeline
```

**Verbose Output:**
```
ğŸ“‚ Pipeline: data_processor
   ğŸ“ Description: Customer Data ETL
   ğŸ‘¤ Author: Data Engineering Team
   ğŸ·ï¸  Tags: etl, customers, daily
   ğŸ“… Version: v2.1.0
   ğŸ“ Location: ./pipelines/data_processor.yaml
   âš™ï¸  Steps: 7 (read_file â†’ parse_json â†’ flatten â†’ validate â†’ format_csv â†’ write_file â†’ backup)
   ğŸ”§ Environment variables: INPUT_PATH, OUTPUT_PATH, BATCH_SIZE
```

### 2. `add` - Create New Pipeline

**Syntax:**
```bash
oxide_flow pipeline add <NAME> [OPTIONS]
```

**Options:**
- `--template <TEMPLATE>` / `-t` - Use specific template (default: "basic")
- `--description <DESC>` / `-d` - Pipeline description
- `--author <AUTHOR>` / `-a` - Pipeline author

**Available Templates:**
- `basic` - Simple read â†’ transform â†’ write pipeline
- `etl` - Extract, Transform, Load pattern
- `validation` - Data validation and quality checking
- `batch` - Batch processing with error handling
- `api` - API data processing
- `streaming` - Streaming data processing

**Features:**
- Interactive pipeline creation wizard
- Template-based pipeline generation
- Pre-filled metadata from project config
- Validation of pipeline name (snake_case enforcement)

**Example Usage:**
```bash
# Basic pipeline creation
oxide_flow pipeline add customer_etl

# With template and description
oxide_flow pipeline add data_validator --template validation --description "Validates incoming customer data"
```

**Interactive Flow:**
```
ğŸ“ Creating new pipeline: customer_etl

ğŸ¯ Select template:
  1. basic      - Simple read â†’ transform â†’ write
  2. etl        - Extract, Transform, Load pattern
  3. validation - Data validation and quality checking
  4. batch      - Batch processing with error handling
  5. api        - API data processing

Enter choice [1-5] (default: 1): 2

ğŸ“‹ Pipeline Details:
  Name: customer_etl
  Description: Customer data ETL pipeline
  Author: Data Engineering Team (from project config)
  Template: etl

âœ… Created pipeline: ./pipelines/customer_etl.yaml

ğŸ’¡ Use 'oxide_flow pipeline test customer_etl' to validate
ğŸš€ Use 'oxide_flow run customer_etl' to execute
```

### 3. `test` - Test/Validate Pipeline

**Syntax:**
```bash
oxide_flow pipeline test <NAME> [OPTIONS]
```

**Options:**
- `--dry-run` - Validate only, don't execute
- `--verbose` / `-v` - Show detailed validation information
- `--fix` - Attempt to fix common issues
- `--schema` - Validate against schemas only

**Features:**
- YAML syntax validation
- Schema validation for all Oxis
- Configuration completeness checking
- Environment variable validation
- Step reference validation
- Dependency checking

**Example Output:**
```bash
ğŸ§ª Testing pipeline: customer_etl

âœ… YAML Syntax: Valid
âœ… Schema Validation: All steps valid
âœ… Environment Variables: All variables available
âœ… Step References: All references valid
âœ… Dependencies: All required Oxis available

ğŸ“Š Pipeline Analysis:
   ğŸ“ˆ Steps: 8 total
   ğŸ”„ Retry-enabled steps: 3
   â° Timeout-configured steps: 2
   ğŸ›¡ï¸  Error-resilient steps: 5
   ğŸ’¾ File operations: 4 read, 2 write
   ğŸŒ Network operations: 1

ğŸ’¡ Suggestions:
   â€¢ Consider adding timeout to step 'api_fetch'
   â€¢ Step 'data_validator' has no retry configuration

âœ… Pipeline is ready for execution
```

**With --fix option:**
```bash
ğŸ§ª Testing pipeline: customer_etl (with auto-fix)

âš ï¸  Issue found: Missing timeout on 'api_fetch' step
   ğŸ”§ Auto-fix: Added timeout_seconds: 30

âš ï¸  Issue found: No retry configuration on 'data_validator'
   ğŸ”§ Auto-fix: Added retry_attempts: 1

âœ… Fixed 2 issues automatically
âœ… Pipeline is ready for execution
```

### 4. `info` - Show Pipeline Information

**Syntax:**
```bash
oxide_flow pipeline info <NAME> [OPTIONS]
```

**Options:**
- `--schema` - Show configuration schema for all steps
- `--json` - Output in JSON format
- `--yaml` - Output in YAML format

**Features:**
- Complete pipeline metadata
- Step-by-step breakdown
- Configuration schema display
- Environment variable requirements
- Performance characteristics

**Example Output:**
```bash
ğŸ“‹ Pipeline Information: customer_etl

ğŸ“ Metadata:
   Name: Customer Data ETL
   Description: Processes daily customer exports from CRM
   Version: 2.1.0
   Author: Data Engineering Team
   Tags: etl, customers, daily, crm
   Created: 2025-01-15
   Location: ./pipelines/customer_etl.yaml

âš™ï¸  Configuration:
   Pipeline Directory: ./pipelines
   Steps: 8 total
   Environment Variables: 5 required, 3 optional

ğŸ”§ Environment Variables:
   Required:
     â€¢ INPUT_PATH - Path to input data file
     â€¢ OUTPUT_PATH - Path for processed output
     â€¢ CRM_API_KEY - API key for CRM access
     â€¢ DATABASE_URL - Database connection string
     â€¢ NOTIFICATION_EMAIL - Email for completion notifications

   Optional:
     â€¢ BATCH_SIZE (default: 1000) - Processing batch size
     â€¢ TIMEOUT (default: 300) - Processing timeout seconds
     â€¢ RETRY_COUNT (default: 3) - Number of retry attempts

ğŸ“Š Steps:
   1. crm_data_fetch (read_file)
      â””â”€ Fetches customer data from CRM export
      â””â”€ Retry: 3 attempts, Timeout: 60s

   2. data_parser (parse_json)
      â””â”€ Parses JSON customer records
      â””â”€ Continue on error: false

   3. data_validator (custom_validator)
      â””â”€ Validates customer data quality
      â””â”€ Retry: 1 attempt

   4. data_transformer (flatten)
      â””â”€ Flattens nested customer objects
      â””â”€ Continue on error: true

   5. csv_formatter (format_csv)
      â””â”€ Formats data as CSV for warehouse
      â””â”€ Headers: true, Delimiter: "|"

   6. warehouse_writer (write_file)
      â””â”€ Writes to data warehouse staging
      â””â”€ Create directories: true

   7. backup_writer (write_file)
      â””â”€ Creates backup copy
      â””â”€ Continue on error: true

   8. notifier (send_notification)
      â””â”€ Sends completion notification
      â””â”€ Continue on error: true

ğŸ”— Dependencies:
   âœ… All required Oxis available
   âœ… All environment variables can be resolved
   âœ… All step references are valid
```

## ğŸ—ï¸ Implementation Plan

### Phase 1: CLI Structure
1. **Update CLI enum** in `src/cli.rs`
   - Add `Pipeline` command with subcommands
   - Define all options and arguments

2. **Add main command handler** in `src/main.rs`
   - Route to appropriate pipeline functions
   - Handle error cases gracefully

### Phase 2: Core Functions
1. **Create `src/pipeline_manager.rs`**
   - Pipeline discovery and listing
   - Metadata extraction and analysis
   - Template management

2. **Implement list functionality**
   - Parse all pipeline files in directory
   - Extract metadata from YAML frontmatter
   - Format and display results

### Phase 3: Pipeline Creation
1. **Template system**
   - Create template directory structure
   - Implement template rendering
   - Interactive pipeline creation wizard

### Phase 4: Testing & Validation
1. **Validation engine**
   - YAML syntax validation
   - Schema validation integration
   - Environment variable checking
   - Step reference validation

2. **Auto-fix capabilities**
   - Common issue detection
   - Automatic corrections
   - User confirmation for changes

### Phase 5: Enhanced Features
1. **Integration features**
   - JSON/YAML output for scripting
   - CI/CD friendly formatting
   - Pipeline dependency analysis

## ğŸ“ File Structure Changes

```
src/
â”œâ”€â”€ main.rs                 # Updated with pipeline command routing
â”œâ”€â”€ cli.rs                  # Updated with pipeline subcommands
â”œâ”€â”€ pipeline_manager.rs     # New: pipeline management functions
â”œâ”€â”€ templates/              # New: pipeline templates
â”‚   â”œâ”€â”€ basic.yaml
â”‚   â”œâ”€â”€ etl.yaml
â”‚   â”œâ”€â”€ validation.yaml
â”‚   â”œâ”€â”€ batch.yaml
â”‚   â””â”€â”€ api.yaml
â””â”€â”€ pipeline.rs            # Enhanced with metadata functions
```

## ğŸ¯ Success Criteria

### User Experience
- âœ… Users can easily discover available pipelines
- âœ… Creating new pipelines is intuitive and fast
- âœ… Pipeline validation catches issues before execution
- âœ… Clear, helpful error messages and suggestions

### Developer Experience
- âœ… Template system makes pipeline creation standardized
- âœ… Auto-fix reduces manual configuration errors
- âœ… Detailed validation helps debug complex pipelines
- âœ… Integration-friendly output for CI/CD

### Performance
- âœ… Pipeline listing is fast (<100ms for 100 pipelines)
- âœ… Validation completes quickly (<500ms per pipeline)
- âœ… Template generation is near-instantaneous

## ğŸš€ Example Usage Workflows

### Discovery Workflow
```bash
# List all pipelines
oxide_flow pipeline list

# Filter by tag
oxide_flow pipeline list --tags etl,daily

# Get detailed info
oxide_flow pipeline info customer_etl

# Test before running
oxide_flow pipeline test customer_etl

# Execute
oxide_flow run customer_etl
```

### Development Workflow
```bash
# Create new pipeline from template
oxide_flow pipeline add new_processor --template etl

# Test during development
oxide_flow pipeline test new_processor --verbose

# Fix issues automatically
oxide_flow pipeline test new_processor --fix

# Final validation
oxide_flow pipeline test new_processor

# Run when ready
oxide_flow run new_processor
```

### CI/CD Integration
```bash
# List pipelines in JSON for processing
oxide_flow pipeline list --json > available_pipelines.json

# Validate all pipelines
for pipeline in $(oxide_flow pipeline list --json | jq -r '.[].name'); do
    oxide_flow pipeline test "$pipeline" || exit 1
done

# Run specific pipeline
oxide_flow run production_etl
```

This comprehensive pipeline management system will make Oxide Flow much more user-friendly and provide professional-grade pipeline development tools!
